import requests
import json
import pandas as pd
from datetime import datetime
import os
import logging
from urllib.parse import urlparse, urljoin
import time
import random
from typing import List, Dict, Optional
import re

# ConfiguraÃ§Ã£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('jina_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class JinaAIRealEstateScraper:
    """
    Scraper de imÃ³veis usando Jina AI Reader API
    - Gratuito e sem necessidade de API key
    - Converte qualquer URL em texto limpo para LLM
    - Rate limit: 20 req/min sem API key, 200 req/min com API key gratuita
    """
    
    def __init__(self, api_key: Optional[str] = None):
        self.base_url = "https://r.jina.ai/"
        self.api_key = api_key
        self.session = requests.Session()
        
        # Headers para requisiÃ§Ãµes
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/plain, application/json',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8'
        }
        
        # Adiciona API key se fornecida (para rate limit maior)
        if self.api_key:
            headers['Authorization'] = f'Bearer {self.api_key}'
            logger.info("âœ… API key configurada - Rate limit: 200 req/min")
        else:
            logger.info("âš ï¸ Sem API key - Rate limit: 20 req/min")
        
        self.session.headers.update(headers)
        
        # ConfiguraÃ§Ãµes de rate limiting
        self.requests_per_minute = 200 if self.api_key else 20
        self.min_delay = 60 / self.requests_per_minute  # segundos entre requisiÃ§Ãµes
        self.last_request_time = 0

    def respect_rate_limit(self):
        """Implementa rate limiting para nÃ£o exceder os limites da API"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_delay:
            sleep_time = self.min_delay - time_since_last
            logger.info(f"â³ Aguardando {sleep_time:.1f}s para respeitar rate limit...")
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()

    def extract_clean_content(self, url: str) -> Optional[str]:
        """
        Extrai conteÃºdo limpo de uma URL usando Jina AI Reader
        
        Args:
            url: URL para extrair conteÃºdo
            
        Returns:
            Texto limpo da pÃ¡gina ou None se erro
        """
        try:
            self.respect_rate_limit()
            
            # URL da Jina AI Reader
            jina_url = f"{self.base_url}{url}"
            
            logger.info(f"ğŸ“– Extraindo conteÃºdo de: {url}")
            
            # Faz requisiÃ§Ã£o para Jina AI
            response = self.session.get(jina_url, timeout=30)
            response.raise_for_status()
            
            # O conteÃºdo vem como texto limpo
            clean_content = response.text
            
            if len(clean_content) > 100:  # Verifica se tem conteÃºdo substancial
                logger.info(f"âœ… ConteÃºdo extraÃ­do: {len(clean_content)} caracteres")
                return clean_content
            else:
                logger.warning(f"âš ï¸ ConteÃºdo muito pequeno: {len(clean_content)} caracteres")
                return None
                
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Erro ao extrair conteÃºdo de {url}: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"âŒ Erro inesperado: {str(e)}")
            return None

    def extract_property_data_with_llm(self, clean_content: str, url: str) -> List[Dict]:
        """
        Extrai dados de imÃ³veis do conteÃºdo limpo usando LLM local (Ollama)
        
        Args:
            clean_content: ConteÃºdo limpo da pÃ¡gina
            url: URL original
            
        Returns:
            Lista de dicionÃ¡rios com dados dos imÃ³veis
        """
        try:
            # Prompt otimizado para extraÃ§Ã£o de imÃ³veis
            prompt = f"""
            Analise o seguinte conteÃºdo de uma pÃ¡gina de imÃ³veis e extraia TODOS os imÃ³veis encontrados.
            Retorne APENAS um JSON vÃ¡lido seguindo esta estrutura EXATA:

            {{
                "imoveis": [
                    {{
                        "titulo": "tÃ­tulo do imÃ³vel",
                        "preco": "preÃ§o (ex: R$ 850.000)",
                        "endereco": "endereÃ§o completo (SEM bairro e cidade)",
                        "bairro": "bairro (extrair do endereÃ§o se necessÃ¡rio)", 
                        "cidade": "cidade (extrair do endereÃ§o se necessÃ¡rio)",
                        "area_m2": "Ã¡rea em mÂ² (apenas nÃºmero)",
                        "quartos": "nÃºmero de quartos (apenas nÃºmero)",
                        "banheiros": "nÃºmero de banheiros (apenas nÃºmero)",
                        "vagas": "nÃºmero de vagas (apenas nÃºmero)",
                        "tipo": "tipo do imÃ³vel",
                        "condominio": "valor do condomÃ­nio",
                        "caracteristicas": ["lista de caracterÃ­sticas"],
                        "link": "URL do imÃ³vel se encontrada"
                    }}
                ]
            }}

            REGRAS IMPORTANTES:
            - Extraia TODOS os imÃ³veis da pÃ¡gina
            - Para campos nÃ£o encontrados, use null
            - Para nÃºmeros, use apenas o valor numÃ©rico
            - URLs devem ser completas
            - NUNCA inclua texto explicativo, apenas o JSON
            - EndereÃ§o NÃƒO deve conter bairro e cidade (devem ir em campos separados)
            - Se bairro/cidade estiverem no endereÃ§o, extraia para os campos corretos

            CONTEÃšDO DA PÃGINA:
            {clean_content[:8000]}  # Limita para nÃ£o exceder contexto
            """

            # Faz requisiÃ§Ã£o para Ollama local
            ollama_url = "http://localhost:11434/api/generate"
            payload = {
                "model": "llama3:8b-instruct-q4_K_M",
                "prompt": prompt,
                "format": "json",
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_ctx": 8192
                }
            }

            logger.info("ğŸ¤– Processando com Ollama...")
            response = requests.post(ollama_url, json=payload, timeout=120)
            
            if response.status_code == 200:
                result = response.json()
                llm_response = result.get('response', '')
                
                # Parseia o JSON retornado
                try:
                    properties_data = json.loads(llm_response)
                    properties = properties_data.get('imoveis', [])
                    
                    # PÃ³s-processamento para garantir separaÃ§Ã£o de endereÃ§o/bairro/cidade
                    for prop in properties:
                        self.clean_address_fields(prop)
                    
                    logger.info(f"âœ… {len(properties)} imÃ³veis extraÃ­dos pelo LLM")
                    return properties
                    
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ Erro ao parsear JSON do LLM: {str(e)}")
                    # Tenta extrair JSON vÃ¡lido da resposta
                    return self.extract_json_from_text(llm_response)
            else:
                logger.error(f"âŒ Erro no Ollama: {response.status_code}")
                return []

        except requests.exceptions.ConnectionError:
            logger.error("âŒ Ollama nÃ£o estÃ¡ rodando. Execute: ollama serve")
            return []
        except Exception as e:
            logger.error(f"âŒ Erro no processamento LLM: {str(e)}")
            return []

    def clean_address_fields(self, property_data: Dict):
        """Garante que endereÃ§o, bairro e cidade estejam corretamente separados"""
        if 'endereco' in property_data and property_data['endereco']:
            address = property_data['endereco']
            
            # Se bairro estÃ¡ vazio mas pode estar no endereÃ§o
            if not property_data.get('bairro') and ',' in address:
                parts = [p.strip() for p in address.split(',')]
                if len(parts) > 1:
                    property_data['bairro'] = parts[-1]
                    property_data['endereco'] = ','.join(parts[:-1])
            
            # Se cidade estÃ¡ vazia mas pode estar no endereÃ§o
            if not property_data.get('cidade') and '-' in address:
                parts = [p.strip() for p in address.split('-')]
                if len(parts) > 1:
                    property_data['cidade'] = parts[-1]
                    property_data['endereco'] = '-'.join(parts[:-1])

    def extract_json_from_text(self, text: str) -> List[Dict]:
        """Tenta extrair JSON vÃ¡lido de um texto que pode conter outros elementos"""
        try:
            # Procura por padrÃ£o JSON
            start_idx = text.find('{')
            end_idx = text.rfind('}')
            
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = text[start_idx:end_idx+1]
                data = json.loads(json_str)
                properties = data.get('imoveis', [])
                
                # Aplica limpeza de campos de endereÃ§o
                for prop in properties:
                    self.clean_address_fields(prop)
                
                return properties
        except:
            pass
        
        return []

    def extract_properties_simple(self, clean_content: str, url: str) -> List[Dict]:
        """
        ExtraÃ§Ã£o simples usando regex (fallback se Ollama nÃ£o estiver disponÃ­vel)
        """
        properties = []
        
        try:
            # PadrÃµes regex para detectar informaÃ§Ãµes de imÃ³veis
            price_pattern = r'R\$\s*[\d.,]+(?:\s*mil)?'
            room_pattern = r'(\d+)\s*(?:quarto|dormitÃ³rio|bedroom)'
            bath_pattern = r'(\d+)\s*(?:banheiro|bathroom|wc)'
            area_pattern = r'(\d+(?:[.,]\d+)?)\s*m[Â²2]'
            
            # Divide o conteÃºdo em seÃ§Ãµes (assumindo que cada imÃ³vel Ã© uma seÃ§Ã£o)
            sections = re.split(r'\n\s*\n', clean_content)
            
            for i, section in enumerate(sections):
                if len(section) < 50:  # Pula seÃ§Ãµes muito pequenas
                    continue
                    
                # Busca por preÃ§os na seÃ§Ã£o
                prices = re.findall(price_pattern, section, re.IGNORECASE)
                if not prices:
                    continue
                
                # Extrai informaÃ§Ãµes bÃ¡sicas
                property_data = {
                    'titulo': self.extract_title_from_section(section),
                    'preco': prices[0] if prices else None,
                    'endereco': None,
                    'bairro': None,
                    'cidade': None,
                    'area_m2': None,
                    'quartos': None,
                    'banheiros': None,
                    'vagas': None,
                    'tipo': None,
                    'condominio': None,
                    'caracteristicas': [],
                    'link': url
                }
                
                # Extrai endereÃ§o, bairro e cidade
                address_info = self.extract_address_from_section(section)
                if address_info:
                    property_data['endereco'] = address_info.get('endereco')
                    property_data['bairro'] = address_info.get('bairro')
                    property_data['cidade'] = address_info.get('cidade')
                
                # Extrai quartos
                rooms = re.findall(room_pattern, section, re.IGNORECASE)
                if rooms:
                    property_data['quartos'] = int(rooms[0])
                
                # Extrai banheiros
                baths = re.findall(bath_pattern, section, re.IGNORECASE)
                if baths:
                    property_data['banheiros'] = int(baths[0])
                
                # Extrai Ã¡rea
                areas = re.findall(area_pattern, section, re.IGNORECASE)
                if areas:
                    property_data['area_m2'] = float(areas[0].replace(',', '.'))
                
                properties.append(property_data)
                
                # Limita a 20 imÃ³veis para nÃ£o sobrecarregar
                if len(properties) >= 20:
                    break
            
            logger.info(f"âœ… {len(properties)} imÃ³veis extraÃ­dos com regex")
            return properties
            
        except Exception as e:
            logger.error(f"âŒ Erro na extraÃ§Ã£o simples: {str(e)}")
            return []

    def extract_title_from_section(self, section: str) -> str:
        """Extrai tÃ­tulo provÃ¡vel de uma seÃ§Ã£o"""
        lines = section.split('\n')
        for line in lines:
            line = line.strip()
            if len(line) > 10 and len(line) < 100:
                # Verifica se parece com um tÃ­tulo
                if any(word in line.lower() for word in ['apartamento', 'casa', 'imÃ³vel', 'venda', 'aluguel']):
                    return line
        
        # Se nÃ£o encontrar, usa a primeira linha nÃ£o vazia
        for line in lines:
            line = line.strip()
            if len(line) > 5:
                return line[:80]
        
        return "TÃ­tulo nÃ£o identificado"

    def extract_address_from_section(self, section: str) -> Optional[Dict]:
        """Extrai endereÃ§o, bairro e cidade de uma seÃ§Ã£o"""
        # PadrÃµes comuns de endereÃ§o
        address_pattern = r'(?:Rua|Av|Avenida|Alameda|Travessa|PraÃ§a)\s+[^,]+(?:,\s*\d+)?(?:,\s*[^-]+)?(?:-\s*[^,]+)?(?:,\s*[^-]+)?'
        matches = re.findall(address_pattern, section, re.IGNORECASE)
        
        if not matches:
            return None
            
        full_address = matches[0].strip()
        result = {'endereco': full_address, 'bairro': None, 'cidade': None}
        
        # Tenta extrair bairro e cidade
        parts = [p.strip() for p in full_address.split(',')]
        if len(parts) > 1:
            result['bairro'] = parts[-1]
            result['endereco'] = ','.join(parts[:-1])
            
            # Verifica se tem cidade apÃ³s hÃ­fen
            if '-' in result['bairro']:
                city_parts = result['bairro'].split('-')
                if len(city_parts) > 1:
                    result['cidade'] = city_parts[-1].strip()
                    result['bairro'] = city_parts[0].strip()
        
        return result

    def process_url(self, url: str, use_llm: bool = True) -> List[Dict]:
        """
        Processa uma URL completa: extrai conteÃºdo + processa com LLM
        
        Args:
            url: URL para processar
            use_llm: Se deve usar LLM (Ollama) para processamento
            
        Returns:
            Lista de imÃ³veis extraÃ­dos
        """
        logger.info(f"ğŸ  Processando URL: {url}")
        
        # 1. Extrai conteÃºdo limpo com Jina AI
        clean_content = self.extract_clean_content(url)
        if not clean_content:
            logger.error("âŒ NÃ£o foi possÃ­vel extrair conteÃºdo da URL")
            return []
        
        # 2. Processa conteÃºdo com LLM ou regex
        if use_llm:
            properties = self.extract_property_data_with_llm(clean_content, url)
            
            # Se LLM falhar, usa mÃ©todo simples
            if not properties:
                logger.info("ğŸ”„ LLM falhou, tentando extraÃ§Ã£o simples...")
                properties = self.extract_properties_simple(clean_content, url)
        else:
            properties = self.extract_properties_simple(clean_content, url)
        
        # 3. Adiciona metadados
        for prop in properties:
            prop['site_origem'] = urlparse(url).netloc
            prop['data_coleta'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            prop['url_origem'] = url
        
        return properties

    def process_multiple_urls(self, urls: List[str], use_llm: bool = True) -> List[Dict]:
        """
        Processa mÃºltiplas URLs
        
        Args:
            urls: Lista de URLs para processar
            use_llm: Se deve usar LLM
            
        Returns:
            Lista combinada de todos os imÃ³veis
        """
        all_properties = []
        
        logger.info(f"ğŸ”„ Processando {len(urls)} URLs...")
        
        for i, url in enumerate(urls, 1):
            logger.info(f"ğŸ“ URL {i}/{len(urls)}: {url}")
            
            properties = self.process_url(url, use_llm)
            all_properties.extend(properties)
            
            logger.info(f"âœ… {len(properties)} imÃ³veis encontrados nesta URL")
            
            # Pausa entre URLs para ser respeitoso
            if i < len(urls):
                time.sleep(random.uniform(1, 3))
        
        logger.info(f"ğŸ‰ Total: {len(all_properties)} imÃ³veis de todas as URLs")
        return all_properties

    def save_to_csv(self, properties: List[Dict], filename: Optional[str] = None) -> str:
        """
        Salva imÃ³veis em arquivo CSV
        
        Args:
            properties: Lista de imÃ³veis
            filename: Nome do arquivo (opcional)
            
        Returns:
            Path do arquivo salvo
        """
        if not properties:
            logger.warning("âš ï¸ Nenhum imÃ³vel para salvar")
            return ""
        
        try:
            # Cria DataFrame
            df = pd.DataFrame(properties)
            
            # Remove duplicatas baseado em tÃ­tulo + preÃ§o
            initial_count = len(df)
            df = df.drop_duplicates(subset=['titulo', 'preco'], keep='first')
            final_count = len(df)
            
            if initial_count > final_count:
                logger.info(f"ğŸ”„ Removidas {initial_count - final_count} duplicatas")
            
            # Reorganiza colunas (removendo descriÃ§Ã£o)
            priority_cols = ['titulo', 'preco', 'endereco', 'bairro', 'cidade', 
                           'area_m2', 'quartos', 'banheiros', 'vagas', 'tipo', 'condominio']
            
            existing_priority = [col for col in priority_cols if col in df.columns]
            other_cols = [col for col in df.columns if col not in existing_priority and col != 'descricao']
            df = df[existing_priority + other_cols]
            
            # Cria diretÃ³rio
            os.makedirs('resultados_jina', exist_ok=True)
            
            # Nome do arquivo
            if not filename:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"resultados_jina/imoveis_jina_{timestamp}.csv"
            
            # Salva CSV
            df.to_csv(filename, index=False, encoding='utf-8-sig', sep=';')
            
            logger.info(f"ğŸ’¾ CSV salvo: {filename} ({len(df)} imÃ³veis)")
            return filename
            
        except Exception as e:
            logger.error(f"âŒ Erro ao salvar CSV: {str(e)}")
            return ""

    def get_sample_urls(self) -> List[str]:
        """Retorna URLs de exemplo para teste"""
        return [
            "https://www.chavesnamao.com.br/apartamentos-venda-sao-paulo-sp",
            "https://www.123i.com.br/apartamento-venda-sao-paulo-sp",
            "https://casa.com.br/venda/sp/sao-paulo/apartamento",
            "https://www.wimoveis.com.br/apartamentos-venda-sao-paulo-sp.html",
            "https://www.imovelweb.com.br/apartamentos-venda-sao-paulo-sp.html"
        ]

def main():
    """FunÃ§Ã£o principal"""
    print("\n" + "="*70)
    print("ğŸ  SCRAPER DE IMÃ“VEIS COM JINA AI - v1.1")
    print("   Powered by Jina AI Reader API (Gratuito!)")
    print("="*70)
    
    # Pergunta sobre API key
    print("\nğŸ”‘ API KEY DA JINA AI (OPCIONAL)")
    print("   â€¢ Sem API key: 20 requisiÃ§Ãµes/min (gratuito)")
    print("   â€¢ Com API key gratuita: 200 requisiÃ§Ãµes/min")
    print("   â€¢ Para obter API key: https://jina.ai/reader/")
    
    api_key = input("\nDigite sua API key da Jina AI (ou Enter para pular): ").strip()
    api_key = api_key if api_key else None
    
    # Inicializa scraper
    scraper = JinaAIRealEstateScraper(api_key=api_key)
    
    # Pergunta sobre LLM
    print("\nğŸ¤– PROCESSAMENTO DE DADOS")
    print("1. ğŸ§  Com Ollama (melhor qualidade - requer ollama local)")
    print("2. ğŸ“ Regex simples (funciona sem dependÃªncias)")
    
    llm_choice = input("\nEscolha (1 ou 2): ").strip()
    use_llm = llm_choice == "1"
    
    if use_llm:
        print("\nâš ï¸  CERTIFIQUE-SE QUE O OLLAMA ESTÃ RODANDO:")
        print("   ollama serve")
        print("   ollama pull llama3:8b-instruct-q4_K_M")
    
    # URLs de exemplo
    sample_urls = scraper.get_sample_urls()
    print(f"\nğŸ’¡ URLS DE EXEMPLO:")
    for i, url in enumerate(sample_urls, 1):
        print(f"   {i}. {url}")
    
    print(f"\nğŸ’¡ DICA: Use URLs especÃ­ficas de busca para melhores resultados!")
    
    while True:
        try:
            print(f"\n" + "="*50)
            urls_input = input("Digite URL(s) separadas por vÃ­rgula (ou 'sair'): ").strip()
            
            if urls_input.lower() in ('sair', 'exit', 'quit', 's'):
                print("ğŸ‘‹ Encerrando scraper...")
                break
            
            if not urls_input:
                continue
            
            # Processa URLs
            urls = [url.strip() for url in urls_input.split(',') if url.strip()]
            
            # Valida URLs
            valid_urls = []
            for url in urls:
                if not url.startswith(('http://', 'https://')):
                    url = 'https://' + url
                
                try:
                    parsed = urlparse(url)
                    if parsed.netloc:
                        valid_urls.append(url)
                    else:
                        print(f"âŒ URL invÃ¡lida: {url}")
                except:
                    print(f"âŒ URL invÃ¡lida: {url}")
            
            if not valid_urls:
                print("âŒ Nenhuma URL vÃ¡lida fornecida")
                continue
            
            print(f"\nğŸš€ Iniciando scraping de {len(valid_urls)} URL(s)...")
            print(f"ğŸ”§ MÃ©todo: {'Jina AI + Ollama' if use_llm else 'Jina AI + Regex'}")
            
            # Processa URLs
            all_properties = scraper.process_multiple_urls(valid_urls, use_llm)
            
            if all_properties:
                # Salva CSV
                csv_file = scraper.save_to_csv(all_properties)
                
                # RelatÃ³rio final
                print(f"\n{'='*60}")
                print(f"ğŸ‰ SCRAPING CONCLUÃDO!")
                print(f"{'='*60}")
                print(f"ğŸ“Š Total de imÃ³veis: {len(all_properties)}")
                print(f"ğŸŒ URLs processadas: {len(valid_urls)}")
                if csv_file:
                    print(f"ğŸ’¾ Arquivo CSV: {os.path.abspath(csv_file)}")
                
                # Preview dos dados
                if all_properties:
                    print(f"\nğŸ“‹ PREVIEW DOS DADOS:")
                    print(f"{'â”€'*40}")
                    
                    for i, prop in enumerate(all_properties[:3], 1):
                        print(f"ğŸ  ImÃ³vel {i}:")
                        if prop.get('titulo'):
                            print(f"   TÃ­tulo: {prop['titulo'][:60]}...")
                        if prop.get('preco'):
                            print(f"   PreÃ§o: {prop['preco']}")
                        if prop.get('endereco'):
                            print(f"   EndereÃ§o: {prop['endereco']}")
                        if prop.get('bairro'):
                            print(f"   Bairro: {prop['bairro']}")
                        if prop.get('cidade'):
                            print(f"   Cidade: {prop['cidade']}")
                        print()
                
                print(f"ğŸ•’ ConcluÃ­do em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
                print(f"{'='*60}")
                
            else:
                print("âš ï¸  Nenhum imÃ³vel foi extraÃ­do.")
                print("   PossÃ­veis causas:")
                print("   â€¢ URL nÃ£o contÃ©m listagem de imÃ³veis")
                print("   â€¢ Site tem proteÃ§Ã£o anti-scraping")
                print("   â€¢ ConteÃºdo nÃ£o foi reconhecido como imÃ³veis")
                
        except KeyboardInterrupt:
            print("\n\nâ›” OperaÃ§Ã£o cancelada pelo usuÃ¡rio")
            break
        except Exception as e:
            print(f"\nâŒ ERRO INESPERADO: {str(e)}")
            logger.error(f"Erro nÃ£o tratado: {str(e)}")
            continue
    
    print(f"\nğŸ Scraper encerrado.")
    print(f"ğŸ“ Verifique a pasta 'resultados_jina' para os arquivos CSV.")
    print(f"\nğŸ“‹ SOBRE A JINA AI:")
    print(f"   âœ… Completamente gratuita (atÃ© 20 req/min)")
    print(f"   ğŸ§¹ Extrai apenas conteÃºdo relevante")
    print(f"   ğŸš€ Funciona com qualquer site")
    print(f"   ğŸ“Š Ideal para alimentar LLMs")

if __name__ == "__main__":
    main()
