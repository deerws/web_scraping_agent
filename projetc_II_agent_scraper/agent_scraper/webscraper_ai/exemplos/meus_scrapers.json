[
  {
    "objetivo": "Extrair dados de imóveis em leilão no site Superbid",
    "url": "https://www.superbid.net/categorias/imoveis?searchType=opened",
    "tecnologia_usada": "Selenium + BeautifulSoup + ThreadPoolExecutor",
    "codigo": "from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport time, traceback\nfrom bs4 import BeautifulSoup\nimport re\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport queue\nimport threading\n\nclass SuperbidScraper:\n    def __init__(self, max_workers=6):\n        self.options = webdriver.ChromeOptions()\n        self.options.add_argument('--headless=new')\n        self.options.add_argument('--disable-gpu')\n        self.options.add_argument('--no-sandbox')\n        self.options.add_argument('--disable-dev-shm-usage')\n        self.options.add_argument('--disable-blink-features=AutomationControlled')\n        self.options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n        self.options.add_experimental_option('useAutomationExtension', False)\n\n        self.url = \"https://www.superbid.net/categorias/imoveis?searchType=opened\"\n        self.url_base = \"https://www.superbid.net\"\n        self.max_workers = max_workers\n        self.driver = None\n        self.unique_links = set()\n        self.property_data = []\n        self.lock = threading.Lock()\n        self.task_queue = queue.Queue()\n        self.results = []\n        self.drivers = []\n        self.all_characteristics = set()\n\n    def init_driver(self):\n        driver = webdriver.Chrome(options=self.options)\n        self.drivers.append(driver)\n        return driver\n\n    def get_homelinks(self):\n        try:\n            self.driver = self.init_driver()\n            self.driver.get(self.url)\n            time.sleep(5)\n            \n            try:\n                cookie_btn = WebDriverWait(self.driver, 10).until(\n                    EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Aceitar')]\"))\n                )\n                cookie_btn.click()\n                time.sleep(1)\n            except:\n                pass\n            \n            while True:\n                self.process_current_page()\n                if not self.go_to_next_page():\n                    break\n                time.sleep(3)\n                \n            print(f\"\\nTotal de imóveis encontrados: {len(self.unique_links)}\")\n            \n            for link in self.unique_links:\n                self.task_queue.put(link)\n            \n        except Exception as e:\n            print(f\"Erro no get_homelinks: {e}\")\n            traceback.print_exc()\n\n    def process_current_page(self):\n        try:\n            WebDriverWait(self.driver, 20).until(\n                EC.presence_of_element_located((By.CSS_SELECTOR, \"a[id^='offer-card-']\"))\n            )\n            soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n            \n            for a in soup.find_all(\"a\", id=lambda x: x and x.startswith(\"offer-card-\")):\n                if href := a.get(\"href\"):\n                    with self.lock:\n                        self.unique_links.add(self.url_base + href)\n            \n        except Exception as e:\n            print(f\"Erro no process_current_page: {e}\")\n\n    def go_to_next_page(self):\n        try:\n            next_btn = WebDriverWait(self.driver, 10).until(\n                EC.presence_of_element_located((By.XPATH, \"//button[contains(., 'Próximo') and not(contains(@class, 'disabled'))]\"))\n            )\n            self.driver.execute_script(\"arguments[0].click();\", next_btn)\n            WebDriverWait(self.driver, 10).until(EC.staleness_of(next_btn))\n            time.sleep(3)\n            return True\n        except:\n            print(\"Não há mais páginas\")\n            return False\n\n    def worker(self):\n        driver = self.init_driver()\n        while True:\n            try:\n                url = self.task_queue.get_nowait()\n                property_info = self.get_property_info(url, driver)\n                if property_info:\n                    with self.lock:\n                        for key in property_info.keys():\n                            if key.startswith('caract_') or key.startswith('valor_'):\n                                self.all_characteristics.add(key)\n                        self.property_data.append(property_info)\n                self.task_queue.task_done()\n            except queue.Empty:\n                break\n\n    def get_property_info(self, url, driver):\n        data = {\n            \"url\": url,\n            \"titulo\": None,\n            \"endereco_completo\": None,\n            \"ultimo_lance\": None,\n            \"leiloeiro\": None,\n            \"vendido_por\": None,\n            \"descricao\": None,\n        }\n\n        try:\n            driver.get(url)\n            time.sleep(3)\n            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n            time.sleep(2)\n\n            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n\n            if title := soup.find(\"h1\"):\n                data[\"titulo\"] = title.get_text(strip=True)\n\n            if loc_div := soup.find(\"div\", class_=\"sc-8126a53f-4\"):\n                data[\"endereco_completo\"] = loc_div.get_text(separator=\" | \", strip=True)\n\n            if lance := soup.find(\"span\", class_=\"lance-atual\"):\n                data[\"ultimo_lance\"] = self.clean_value(lance.get_text(strip=True))\n\n            div_dados_leilao = soup.find_all(\"div\", class_=\"sc-8126a53f-3 jZSJxj\")\n            for dado in div_dados_leilao:\n                titles = dado.find_all(\"p\", class_=\"sc-8126a53f-6 uegjp\")\n                values = dado.find_all(\"p\", class_=\"sc-8126a53f-7 fygozL\")\n                for title, value in zip(titles, values):\n                    t = title.get_text(strip=True)\n                    v = value.get_text(strip=True)\n                    if \"Vendido por\" in t:\n                        data[\"vendido_por\"] = v\n                    elif \"Leiloeiro\" in t:\n                        data[\"leiloeiro\"] = v\n\n            try:\n                driver.find_element(By.XPATH, \"//button[contains(., 'Continuar lendo')]\").click()\n                time.sleep(1)\n            except:\n                pass\n            \n            if desc_div := soup.find(\"div\", class_=\"sc-8126a53f-13\"):\n                desc_text = desc_div.get_text(separator=\"\\n\", strip=True)\n                desc_data = self.parse_description(desc_text)\n                for key, value in desc_data.items():\n                    clean_key = self.clean_column_name(f\"desc_{key}\")\n                    data[clean_key] = value\n                \n            characteristics = self.extract_structured_section(driver, \"Características do Imóvel\")\n            for key, value in characteristics.items():\n                clean_key = self.clean_column_name(f\"caract_{key}\")\n                data[clean_key] = value\n                \n            values = self.extract_structured_section(driver, \"Valores\")\n            for key, value in values.items():\n                clean_key = self.clean_column_name(f\"valor_{key}\")\n                data[clean_key] = value\n                \n            print(f\"Processado: {url}\")\n            return data\n\n        except Exception as e:\n            print(f\"Erro ao processar {url}: {e}\")\n            traceback.print_exc()\n            return None\n\n    def parse_description(self, description_text):\n        result = {}\n        if not description_text:\n            return result\n\n        patterns = {\n            \"Área Total\": r\"Área [Tt]otal[:]?\\s*([\\d,.]+)\\s*m²\",\n            \"Área Privativa\": r\"Área [Pp]rivativa[:]?\\s*([\\d,.]+)\\s*m²\",\n            \"Área Construída\":r\"Área Construída[:]?\\s*([\\d,.]+)\\s*m²\",\n            \"Status\":r\"Status da obra\",\n            \"Quartos\": r\"Quartos[:]?\\s*(\\d+)\",\n            \"Banheiros\": r\"Banheiros[:]?\\s*(\\d+)\",\n            \"Vagas\": r\"Vagas[:]?\\s*(\\d+)\",\n            \"Andar\": r\"Andar[:]?\\s*(\\d+|Térreo|[\\wçã]+)\",\n            \"Situação\": r\"Situação[:]?\\s*R?\\$?\\s*([\\d,.]+)\",\n            \"Apartamento\": r\"Apartamento[:]?\\s*R?\\$?\\s*([\\d,.]+)\",\n            \"Tipo de Venda\":r\"Tipo de Venda[:]\",\n            \"Torre\":r\"Torre[:]\"\n        }\n\n        for key, pattern in patterns.items():\n            match = re.search(pattern, description_text)\n            if match:\n                result[key] = match.group(1).strip()\n\n        result[\"texto_completo\"] = description_text\n\n        return result\n\n    def extract_structured_section(self, driver, section_title):\n        result = {}\n        try:\n            WebDriverWait(driver, 10).until(\n                lambda d: d.execute_script(\"return document.readyState === 'complete'\"))\n            \n            try:\n                button = WebDriverWait(driver, 5).until(\n                    EC.presence_of_element_located((By.XPATH, \n                        f\"//*[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{section_title.lower()}')]\" +\n                        \"/ancestor-or-self::div[contains(@class, 'sc-')]\"))\n                )\n            except:\n                button = None\n    \n            if not button:\n                try:\n                    buttons = driver.find_elements(By.CSS_SELECTOR, \"div.sc-29469d5b-1.eQnVdT\")\n                    for btn in buttons:\n                        if section_title.lower() in btn.text.lower():\n                            button = btn\n                            break\n                except:\n                    pass\n                \n            if not button:\n                print(f\"Botão '{section_title}' não encontrado - tentando extrair diretamente do HTML\")\n                return self.extract_from_html_fallback(driver, section_title)\n    \n            try:\n                current_state = button.get_attribute(\"data-state\")\n                if current_state == \"closed\":\n                    driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button)\n                    time.sleep(0.5)\n                    driver.execute_script(\"arguments[0].click();\", button)\n                    time.sleep(1)\n            except Exception as e:\n                print(f\"Erro ao interagir com botão: {str(e)}\")\n                return self.extract_from_html_fallback(driver, section_title)\n    \n            content_id = button.get_attribute(\"aria-controls\")\n            content = None\n            \n            if content_id:\n                try:\n                    content = WebDriverWait(driver, 5).until(\n                        EC.visibility_of_element_located((By.ID, content_id)))\n                except:\n                    pass\n                \n            if not content:\n                return self.extract_from_html_fallback(driver, section_title)\n    \n            return self.process_section_content(content.get_attribute(\"innerHTML\"))\n    \n        except Exception as e:\n            print(f\"Erro crítico ao extrair seção: {str(e)}\")\n            return self.extract_from_html_fallback(driver, section_title)\n\n    def extract_from_html_fallback(self, driver, section_title):\n        result = {}\n        try:\n            html = driver.page_source\n            soup = BeautifulSoup(html, \"html.parser\")\n\n            section_header = soup.find(lambda tag: tag.name in ['h2', 'h3', 'div'] and \n                                     section_title.lower() in tag.get_text().lower())\n\n            if not section_header:\n                return result\n\n            section_content = None\n            parent = section_header.find_parent()\n\n            for sibling in section_header.find_next_siblings():\n                if sibling.get('class') and any('sc-' in c for c in sibling.get('class')):\n                    section_content = sibling\n                    break\n\n            if not section_content:\n                return result\n\n            return self.process_section_content(str(section_content))\n\n        except Exception as e:\n            print(f\"Erro no fallback HTML: {str(e)}\")\n            return result\n\n    def process_section_content(self, html_content):\n        result = {}\n        try:\n            soup = BeautifulSoup(html_content, \"html.parser\")\n\n            items = soup.find_all(\"div\", class_=lambda x: x and \"sc-\" in x)\n\n            for item in items:\n                title = item.find([\"p\", \"span\"], class_=lambda x: x and \"title\" in x.lower())\n                value = item.find([\"p\", \"span\"], class_=lambda x: x and \"value\" in x.lower())\n\n                if title and value:\n                    key = title.get_text(strip=True).replace(\":\", \"\")\n                    result[key] = value.get_text(strip=True)\n                else:\n                    text = item.get_text(separator=\":\", strip=True)\n                    if \":\" in text:\n                        parts = [p.strip() for p in text.split(\":\", 1)]\n                        if len(parts) == 2:\n                            result[parts[0]] = parts[1]\n\n        except Exception as e:\n            print(f\"Erro ao processar conteúdo: {str(e)}\")\n\n        return result\n\n    def clean_column_name(self, name):\n        cleaned = re.sub(r'[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ _-]', '', name)\n        cleaned = cleaned.replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n        return cleaned.strip('_')\n\n    def clean_value(self, text):\n        if not text:\n            return None\n        return text.replace(\"R$\", \"\").replace(\".\", \"\").replace(\",\", \".\").strip()\n\n    def run_parallel(self):\n        print(\"Iniciando coleta paralela...\")\n        self.get_homelinks()\n        \n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            futures = [executor.submit(self.worker) for _ in range(self.max_workers)]\n            for future in as_completed(futures):\n                future.result()\n        \n        print(\"Coleta paralela concluída!\")\n\n    def save_to_csv(self, filename=\"superbid_imoveis.csv\"):\n        import csv\n        try:\n            if not self.property_data:\n                print(\"Nenhum dado para salvar\")\n                return\n            \n            fieldnames = set()\n            for item in self.property_data:\n                fieldnames.update(item.keys())\n            \n            fieldnames.update(self.all_characteristics)\n            \n            standard_fields = ['url', 'titulo', 'endereco_completo', 'ultimo_lance', \n                              'leiloeiro', 'vendido_por', 'descricao']\n            other_fields = sorted(f for f in fieldnames if f not in standard_fields)\n            ordered_fields = standard_fields + other_fields\n            \n            with open(filename, mode='w', newline='', encoding='utf-8') as f:\n                writer = csv.DictWriter(f, fieldnames=ordered_fields)\n                writer.writeheader()\n                \n                for item in self.property_data:\n                    complete_row = {field: item.get(field, \"\") for field in ordered_fields}\n                    writer.writerow(complete_row)\n            \n            print(f\"Dados salvos em {filename} (total: {len(self.property_data)} registros)\")\n            \n        except Exception as e:\n            print(f\"Erro ao salvar CSV: {e}\")\n\n    def close_all_drivers(self):\n        for driver in self.drivers:\n            try:\n                driver.quit()\n            except:\n                pass\n\n    def run(self):\n        try:\n            self.run_parallel()\n            self.save_to_csv()\n        except Exception as e:\n            print(f\"Erro principal: {e}\")\n            traceback.print_exc()\n        finally:\n            self.close_all_drivers()\n\nif __name__ == \"__main__\":\n    scraper = SuperbidScraper(max_workers=4)\n    scraper.run()"
  },
  {
    "objetivo": "Extrair dados de imóveis em leilão no site Sodre Santoro",
    "url": "https://www.sodresantoro.com.br/imoveis/lotes",
    "tecnologia_usada": "Selenium + BeautifulSoup + CircuitBreaker",
    "codigo": "import requests, time, traceback, csv\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom multiprocessing import cpu_count\nfrom urllib.parse import urljoin\nimport random\nfrom lib.req_rules import ReqRules\nfrom lib.circuit_breaker import CircuitBreaker\n\nclass SodreSantoroScraper:\n    def __init__(self, delay=2.0, max_workers=None):\n        self.delay = delay\n        self.max_workers = max_workers or min(10, cpu_count() * 2)\n        self.base_url = \"https://www.sodresantoro.com.br\"\n        self.session = ReqRules.create_requests_session()\n        self.circuit_breaker = CircuitBreaker(max_failures=3, reset_timeout=120)\n        self.last_request_time = time.time()\n\n        options = Options()\n        options.headless = True\n        options.add_argument(\"--disable-gpu\")\n        options.add_argument(\"--no-sandbox\")\n        options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument(\"--window-size=1920,1080\")\n        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n        options.add_experimental_option('useAutomationExtension', False)\n\n        service = Service(\"/usr/bin/chromedriver\")\n        self.driver = webdriver.Chrome(service=service, options=options)\n        self.wait = WebDriverWait(self.driver, 20)\n\n    def __del__(self):\n        if hasattr(self, 'driver'):\n            self.driver.quit()\n\n    def _random_delay(self):\n        time_since_last = time.time() - self.last_request_time\n        sleep_time = random.uniform(self.delay, self.delay + 5.0)\n        if time_since_last < sleep_time:\n            time.sleep(sleep_time - time_since_last)\n        time.sleep(random.uniform(0.5, 2.0))\n        self.last_request_time = time.time()\n\n    def _get_soup(self, url=None, source=None):\n        if source:\n            return BeautifulSoup(source, 'html.parser')\n        try:\n            self._random_delay()\n            response = self.session.get(url, timeout=20)\n            response.raise_for_status()\n            return BeautifulSoup(response.text, 'html.parser')\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 403:\n                print(f\"403 Forbidden error encountered. Rotating session and retrying...\")\n                self.circuit_breaker.record_failure()\n                self.session = ReqRules.create_requests_session()\n                try:\n                    self._random_delay()\n                    response = self.session.get(url, timeout=20)\n                    response.raise_for_status()\n                    return BeautifulSoup(response.text, 'html.parser')\n                except Exception as retry_error:\n                    print(f\"Retry failed for {url}: {retry_error}\")\n                    return None\n            else:\n                print(f\"HTTP error {e.response.status_code} for {url}\")\n                return None\n        except Exception as e:\n            print(f\"Error getting {url}: {e}\")\n            return None\n\n    def _clean_url(self, url):\n        return url if url.startswith('http') else urljoin(self.base_url, url)\n\n    def scrap_item_page(self, url):\n        extra_data = {\n            \"descricao\": \"n/a\",\n            \"forma_pagamento\": \"n/a\",\n            \"metragem\": \"n/a\",\n            \"caracteristicas\": \"n/a\",\n            \"cidade\": \"n/a\",\n            \"bairro\": \"n/a\",\n            \"endereco\": \"n/a\",\n            \"tipo_imovel\": \"n/a\",\n            \"processo_link\": \"n/a\",\n            \"leiloeiro\": \"n/a\"\n        }\n        try:\n            soup = self._get_soup(url)\n            if not soup:\n                return extra_data\n            \n            swiper_wrapper = soup.find(\"div\", {\"data-swiper-target\": \"mainSlider\"})\n            if swiper_wrapper:\n                img_tags = swiper_wrapper.find_all(\"img\", src=True)\n                for i, img in enumerate(img_tags[:3]):\n                    extra_data[f\"imagem{i+1}\"] = img[\"src\"]\n\n            descricao = soup.find(\"div\", id=\"detail_info_lot_description\")\n            if descricao:\n                extra_data[\"descricao\"] = descricao.get_text(strip=True)\n\n            pagamento = soup.find(\"div\", id=\"payments_options\")\n            if pagamento:\n                extra_data[\"forma_pagamento\"] = pagamento.get_text(\" | \", strip=True)\n\n            features_container = soup.find(\"div\", class_=\"grid grid-cols-2 gap-4\")\n            if features_container:\n                features = [f.get_text(strip=True) for f in features_container.find_all(\"div\", recursive=False)]\n                if features:\n                    extra_data[\"metragem\"] = features[0]\n                    extra_data[\"caracteristicas\"] = \" | \".join(features[1:]) if len(features) > 1 else \"n/a\"\n\n            if (cidade := soup.find(\"div\", id=\"detail_info_property_city_state\")):\n                cidade_texto = cidade.get_text(strip=True)\n                extra_data[\"cidade\"] = cidade_texto.split(\":\")[1].strip()\n\n            if (bairro := soup.find(\"div\", id=\"detail_info_property_neighborhood\")):\n                bairro_texto=bairro.get_text(strip=True)\n                extra_data[\"bairro\"] = bairro_texto.split(\":\")[1].strip()\n\n            if (endereco := soup.find(\"div\", id=\"detail_info_property_address\")):\n                endereco_texto=endereco.get_text(strip=True)\n                extra_data[\"endereco\"] = endereco_texto.split(\":\")[1].strip()\n\n            if (tipo_imovel := soup.find(\"div\", id=\"detail_info_property_category\")):\n                tipo_imovel_texto=tipo_imovel.get_text(strip=True)\n                extra_data[\"tipo_imovel\"] = tipo_imovel_texto.split(\":\")[1].strip()\n\n            if (processo := soup.find(\"div\", id=\"aditionalInfoLot_tj_number_process\")):\n                if (processo_link := processo.find(\"a\", href=True)):\n                    extra_data[\"processo_link\"] = processo_link['href']\n\n            if (leiloeiro := soup.find(\"div\", id=\"aditionalInfoLot_leiloeiro\")):\n                leiloeiro_texto=leiloeiro.get_text(strip=True)\n                extra_data[\"leiloeiro\"] = leiloeiro_texto.split(\":\")[1].strip()\n\n            if ocupado := soup.find(\"div\", id=\"extraLabelLot\"):\n                status_tag = ocupado.find(\"span\", string=lambda text: text and (\"Desocupado\" in text or \"Ocupado\" in text))\n\n                if status_tag:\n                    status_text = status_tag.get_text(strip=True)\n                    if \"DESOCUPADO\" in status_text.upper():\n                        extra_data[\"ocupado\"] = \"Desocupado\"\n                    elif \"OCUPADO\" in status_text.upper():\n                        extra_data[\"ocupado\"] = \"Ocupado\"\n                    else:\n                        extra_data[\"ocupado\"] = \"n/a\"\n                else:\n                    extra_data[\"ocupado\"] = \"n/a\"\n\n        except Exception as e:\n            print(f\"Error scraping item page {url}: {e}\")\n            traceback.print_exc()\n\n        return extra_data\n\n    def scrap_main_page(self):\n        results = []\n        links = []\n        page = 1\n        while True:\n            try:\n                url = f\"{self.base_url}/imoveis/lotes?page={page}\"\n                print(f\"Scraping page {page}: {url}\")\n                self.driver.get(url)\n                time.sleep(self.delay)\n\n                self.wait.until(\n                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.relative.flex-none.h-fit.rounded-xl\"))\n                )\n\n                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n                cards = soup.find_all('div', class_=\"relative flex-none h-fit rounded-xl overflow-hidden bg-white border\")\n\n                if not cards:\n                    print(f\"No cards found on page {page}, stopping...\")\n                    break\n\n                for card in cards:\n                    try:\n                        title = card.find('h2', class_=\"text-lg font-bold\")\n                        price = card.find('p', class_=\"text-2xl text-blue-700 font-medium\")\n                        link_tag = card.find(\"a\", href=True)\n\n                        link = self._clean_url(link_tag['href']) if link_tag else None\n\n                        item = {\n                            'url': url,\n                            'preco': price.get_text(strip=True) if price else \"n/a\",\n                            'link': link,\n                        }\n                        results.append(item)\n                        if link:\n                            links.append(link)\n                    except Exception as e:\n                        print(f\"Error processing card on page {page}: {e}\")\n                        continue\n\n                pagination = soup.find('nav', {'aria-label': 'Navegação de Paginação'})\n                if pagination:\n                    page_buttons = pagination.find_all('button')\n                    next_page_buttons = [\n                        btn for btn in page_buttons\n                        if btn.get_text(strip=True).isdigit()\n                        and int(btn.get_text(strip=True)) == page + 1\n                    ]\n\n                    if next_page_buttons:\n                        try:\n                            next_page_btn = self.driver.find_element(\n                                By.XPATH,\n                                f\"//nav[@aria-label='Navegação de Paginação']//button[contains(., '{page + 1}')]\"\n                            )\n                            next_page_btn.click()\n                            time.sleep(self.delay + 2)\n                            page += 1\n                        except Exception as e:\n                            print(f\"Failed to click page {page + 1} button: {e}\")\n                            break\n                    else:\n                        print(\"No next page button found.\")\n                        break\n\n                if page > 50:\n                    print(\"Reached page limit (50), stopping...\")\n                    break\n\n            except Exception as e:\n                print(f\"Error processing page {page}: {e}\")\n                traceback.print_exc()\n                break\n\n        return results, links\n\n    def save_to_csv(self, dados, max_images, filename):\n        fieldnames = [\n            \"url\", \"preco\", \"descricao\", \"forma_pagamento\",\n            \"cidade\", \"bairro\", \"endereco\", \"tipo_imovel\", \n            \"processo_link\", \"leiloeiro\",\"ocupado\"\n        ]\n\n        for i in range(1, max_images + 1):\n            fieldnames.append(f\"imagem{i}\")\n\n        with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writeheader()\n            for d in dados:\n                d.pop(\"link\", None)\n                row = {k: d.get(k, \"n/a\") for k in fieldnames}\n                writer.writerow(row)\n\n    def run(self):\n        try:\n            print(\"Starting main page scraping...\")\n            dados, links = self.scrap_main_page()\n\n            if not dados:\n                print(\"No properties found.\")\n                return {\"error\": \"No properties found.\"}\n\n            print(f\"Found {len(dados)} properties. Scraping details...\")\n            max_images = 0\n            for d, link in zip(dados, links):\n                if link:\n                    extra_info = self.scrap_item_page(link)\n                    img_count = sum(1 for key in extra_info if key.startswith('imagem'))\n                    max_images = max(max_images, img_count)\n                    d.update(extra_info)\n\n            filename = f\"sodresantoro.csv\"\n            self.save_to_csv(dados, max_images, filename)\n            \n            return {\n                \"properties\": dados,\n                \"metadata\": {\n                    \"source\": \"sodresantoro\",\n                    \"scraped_at\": datetime.now().isoformat(),\n                    \"count\": len(dados),\n                    \"output_file\": filename,\n                    \"max_images_found\": max_images\n                }\n            }\n\n        except Exception as e:\n            return {\n                \"error\": str(e),\n                \"traceback\": traceback.format_exc()\n            }\n        finally:\n            if hasattr(self, 'driver'):\n                self.driver.quit()\n            print(\"\\nScraping completed.\\n\")\n\nif __name__ == \"__main__\":\n    scraper = SodreSantoroScraper(delay=2.0)\n    result = scraper.run()\n    print(result.get(\"metadata\", {}))"
  },
  {
    "objetivo": "Extrair dados de imóveis em leilão no site Portalzuk",
    "url": "https://www.portalzuk.com.br/leilao-de-imoveis",
    "tecnologia_usada": "Selenium + BeautifulSoup + TTLCache",
    "codigo": "import csv\nimport random\nimport time\nimport traceback\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom urllib.parse import urljoin, urlparse\nfrom cachetools import TTLCache\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\nfrom    .circuit_breaker  import  CircuitBreaker\n\nclass PortalzukScraper:\n    def __init__(self):\n        self.user_agents = [\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/109.0\",\n            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15\",\n            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/119.0.0.0\",\n            \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:109.0) Gecko/20100101 Firefox/110.0\",\n        ]\n        \n        self.options = webdriver.ChromeOptions()\n        self.options.add_argument('--headless=new')\n        self.options.add_argument('--disable-gpu')\n        self.options.add_argument('--no-sandbox')\n        self.options.add_argument('--disable-dev-shm-usage')\n        self.options.add_argument('--disable-blink-features=AutomationControlled')\n        self.options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n        self.options.add_experimental_option('useAutomationExtension', False)\n        \n        self.current_user_agent = random.choice(self.user_agents)\n        self.options.add_argument(f'user-agent={self.current_user_agent}')\n\n        self.driver = webdriver.Chrome(options=self.options)\n        self.base_url = \"https://www.portalzuk.com.br/leilao-de-imoveis\"\n        self.circuit_breaker = CircuitBreaker(max_failures=3, reset_timeout=120)\n        self.last_request_time = 1\n        self.min_request_interval = 5.0\n        self.max_request_interval_addition = 10.0\n        self.max_workers = 4\n        self.cache = TTLCache(maxsize=1000, ttl=3600)\n        self.session = self._create_requests_session()\n\n    def _create_requests_session(self):\n        session = requests.Session()\n        \n        retry_strategy = Retry(\n            total=5,\n            backoff_factor=2,\n            status_forcelist=[403, 429, 500, 502, 503, 504],\n            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],\n            respect_retry_after_header=True\n        )\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        session.mount(\"https://\", adapter)\n        session.mount(\"http://\", adapter)\n\n        session.headers.update({\n            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n            'Referer': 'https://www.portalzuk.com.br/',\n            'User-Agent': random.choice(self.user_agents),\n            'Accept-Encoding': 'gzip, deflate, br',\n            'Connection': 'keep-alive'\n        })\n        return session\n\n    def random_delay(self):\n        time_since_last = time.time() - self.last_request_time\n        sleep_time = random.uniform(\n            self.min_request_interval, \n            self.min_request_interval + self.max_request_interval_addition\n        )\n        \n        if time_since_last < sleep_time:\n            time.sleep(sleep_time - time_since_last)\n        \n        additional_delay = random.uniform(0.5, 2.0)\n        time.sleep(additional_delay)\n        \n        self.last_request_time = time.time()\n\n    def is_valid_url(self, url):\n        if not url:\n            return False\n        try:\n            result = urlparse(url)\n            return all([result.scheme, result.netloc])\n        except ValueError:\n            return False\n\n    def extract_image_urls(self, html_content):\n        cache_key = f\"image_urls_{hash(html_content)}\"\n        \n        if cache_key in self.cache:\n            return self.cache[cache_key]\n        \n        try:\n            soup = BeautifulSoup(html_content, \"html.parser\")\n            image_urls = []\n            image_tags = soup.select('figure.property-gallery-image img')\n            for img in image_tags:\n                src = img.get('src') or img.get('data-src')\n                if src:\n                    image_urls.append(src)\n            \n            self.cache[cache_key] = image_urls\n            return image_urls\n        except Exception as e:\n            print(f\"Ocorreu um erro ao extrair URLs de imagem: {e}\")\n            traceback.print_exc()\n            return []\n\n    def _scrap_nested_page(self, url):\n        cache_key = f\"nested_page_{url}\"\n        \n        if cache_key in self.cache:\n            return self.cache[cache_key]\n        \n        nested_data = {\n            \"leiloeiro\": None,\n        }\n        if not self.is_valid_url(url):\n            nested_data[\"leiloeiro\"] = \"URL inválida para página aninhada\"\n            return nested_data\n\n        try:\n            self.random_delay()\n            \n            response = self.session.get(url, timeout=15)\n            response.raise_for_status() \n            \n            soup = BeautifulSoup(response.text, \"html.parser\")\n            \n            leiloeiro_label = soup.find(\"h1\", class_=\"whitelabel-title\")\n            if leiloeiro_label:\n                leiloeiro_text = leiloeiro_label.get_text(strip=True)\n                nested_data[\"leiloeiro\"] = leiloeiro_text.replace(\"Leilão de imóveis \", \"\").strip()\n            else:\n                nested_data[\"leiloeiro\"] = \"leiloeiro não encontrado\"\n\n            self.cache[cache_key] = nested_data\n            return nested_data\n\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:\n                retry_after = e.response.headers.get('Retry-After', 30)\n                print(f\"Erro 429 - Esperando {retry_after} segundos antes de tentar novamente...\")\n                time.sleep(int(retry_after))\n                self.circuit_breaker.record_failure()\n                self.session = self._create_requests_session()\n                return self._scrap_nested_page(url)\n            elif e.response.status_code == 403:\n                print(f\"[BLOQUEADO] Erro 403 para URL aninhada {url}: Tentando com nova sessão...\")\n                self.session = self._create_requests_session()\n                try:\n                    response = self.session.get(url, timeout=15)\n                    response.raise_for_status()\n                    soup = BeautifulSoup(response.text, \"html.parser\")\n                    leiloeiro_label = soup.find(\"h1\", class_=\"whitelabel-title\")\n                    if leiloeiro_label:\n                        leiloeiro_text = leiloeiro_label.get_text(strip=True)\n                        nested_data[\"leiloeiro\"] = leiloeiro_text.replace(\"Leilão de imóveis \", \"\").strip()\n                    else:\n                        nested_data[\"leiloeiro\"] = \"leiloeiro não encontrado (re-tentativa)\"\n                    self.cache[cache_key] = nested_data\n                    return nested_data\n                except requests.exceptions.RequestException as retry_e:\n                    print(f\"Erro de requisição mesmo após recriar sessão para URL aninhada {url}: {retry_e}\")\n                    nested_data[\"leiloeiro\"] = \"Erro ao carregar (re-tentativa falhou)\"\n                    return nested_data\n            else:\n                print(f\"Erro HTTP {e.response.status_code} para URL aninhada {url}: {e}\")\n                nested_data[\"leiloeiro\"] = f\"Erro HTTP {e.response.status_code}\"\n                return nested_data\n        except requests.exceptions.RequestException as e:\n            print(f\"Erro de requisição para URL aninhada {url}: {e}\")\n            nested_data[\"leiloeiro\"] = \"Erro de requisição\"\n            return nested_data\n        except Exception as e:\n            print(f\"Erro inesperado ao processar URL aninhada {url}: {e}\")\n            traceback.print_exc()\n            nested_data[\"leiloeiro\"] = \"Erro inesperado\"\n            return nested_data\n\n    def scrapItensPages(self, url):\n        extra_data = {} \n        if not self.is_valid_url(url):\n            print(f\"[AVISO] URL inválida para scrapItensPages: {url}\")\n            return extra_data \n\n        try:\n            self.random_delay()\n            \n            response = self.session.get(url, timeout=20)\n            response.raise_for_status() \n\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            if not soup: \n                print(f\"[ERRO] BeautifulSoup não conseguiu parsear: {url}\")\n                return extra_data\n            \n            features = soup.find_all(\"div\", class_=\"property-featured-item\")\n            for feature in features:\n                label = feature.find(\"span\", class_=\"property-featured-item-label\")\n                value = feature.find(\"span\", class_=\"property-featured-item-value\")\n                if label and value:\n                    extra_data[label.get_text(strip=True)] = value.get_text(strip=True)\n            \n            matricula = soup.find(\"p\", {\"id\": \"itens_matricula\"})\n            if matricula:\n                extra_data[\"Matrícula\"] = matricula.get_text(strip=True)\n            \n            observacoes_geral = soup.find(\"div\", class_=\"div-text-observacoes\")\n            if observacoes_geral:\n                extra_data[\"Observações\"] = observacoes_geral.get_text(strip=True)\n            \n            process_link_found = False\n            box_action_bank_figure = soup.find(\"figure\", class_=\"box-action-bank\")\n            if box_action_bank_figure:\n                process_link_tag = box_action_bank_figure.find(\"a\", href=True)\n                if process_link_tag and self.is_valid_url(process_link_tag['href']):\n                    extra_data[\"Link do Processo Judicial\"] = process_link_tag['href']\n                    process_link_found = True\n            \n            if not process_link_found:\n                processo = soup.find(\"a\", class_=\"glossary-link\")\n                if processo and processo.has_attr(\"href\") and self.is_valid_url(processo['href']):\n                    extra_data[\"Link do Processo Judicial\"] = processo[\"href\"]\n            \n            visitacao_h3 = soup.find(\"h3\", class_=\"property-info-title\", string=lambda text: text and \"Visitação\" in text)\n            if visitacao_h3:\n                visitacao_text_div = visitacao_h3.find_next_sibling(\"div\", class_=\"property-info-text\")\n                if visitacao_text_div:\n                    extra_data[\"Visitação\"] = visitacao_text_div.get_text(strip=True)\n\n            pagamento = soup.find(\"p\", class_=\"property-payments-item-text\")\n            if pagamento:\n                extra_data[\"Formas de Pagamento\"] = pagamento.get_text(strip=True)\n\n            glossary_tags = soup.find_all(\"div\", class_=\"glossary-content\")\n            for tag in glossary_tags:\n                pref_tag = tag.find(\"p\", class_=\"text_subtitle\")\n                if pref_tag and \"DIREITO DE PREFERÊNCIA\" in pref_tag.get_text(strip=True):\n                    extra_data[\"Direito de Preferência\"] = pref_tag.get_text(strip=True)\n\n            description_element_title = soup.find(\"h3\", class_=\"property-info-title\")\n            description_element_text = soup.find(\"p\", class_=\"property-hide-show\")\n            if description_element_title and \"Descrição do imóvel\" in description_element_title.get_text(strip=True):\n                extra_data[\"Descrição do imóvel\"] = description_element_text.get_text(strip=True)\n\n            status_elements = soup.find_all(\"div\", class_=\"property-status\")\n            for status_div in status_elements:\n                title_span = status_div.find(\"span\", class_=\"property-status-title\")\n                text_p = status_div.find(\"p\", class_=\"property-status-text\")\n                if title_span and text_p:\n                    title = title_span.get_text(strip=True)\n                    text = text_p.get_text(strip=True)\n                    \n                    if \"Imóvel ocupado\" in title or \"Imóvel desocupado\" in title:\n                        extra_data[\"ocupado\"] = text\n                    elif \"Direitos do Compromissário Comprador\" in title:\n                        extra_data[\"Direitos do Compromissário\"] = text\n            \n            image_urls = self.extract_image_urls(response.text)\n            for idx, url_img in enumerate(image_urls, start=1): \n                extra_data[f\"Foto_{idx}\"] = url_img\n            extra_data[\"Total_Fotos\"] = len(image_urls)\n\n            documents_div = soup.find(\"div\", class_=\"property-documents-items\")\n            if documents_div:\n                for link in documents_div.find_all('a', class_=\"property-documents-item\"):\n                    label = link.find('span', class_=\"property-documents-item-label\")\n                    if label and label.get_text(strip=True).lower() == \"edital de venda\":\n                        href_edital = link.get('href')\n                        if href_edital and href_edital.strip():\n                            if self.is_valid_url(href_edital):\n                                extra_data[\"Edital de venda\"] = href_edital\n                            else:\n                                print(f\"URL do edital inválida: {href_edital}\")\n                            break\n\n            return extra_data \n\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code in [403, 429]:\n                print(f\"[BLOQUEADO] Erro {e.response.status_code} para {url}: Tentando com nova sessão...\")\n                self.circuit_breaker.record_failure()\n                self.session = self._create_requests_session()\n                return self.scrapItensPages(url)\n            else:\n                print(f\"Erro HTTP {e.response.status_code} para {url}: {e}\")\n                return extra_data\n        except requests.exceptions.RequestException as e:\n            print(f\"Erro de requisição para {url}: {e}\")\n            return extra_data\n        except Exception as e:\n            print(f\"Erro inesperado ao processar {url}: {e}\")\n            traceback.print_exc()\n            return extra_data\n\n    def scrapMainPage(self, html):\n        properties = []\n        try:\n            soup = BeautifulSoup(html, \"html.parser\")\n            cards = soup.find_all(\"div\", class_=\"card-property\")\n\n            for card in cards:\n                try:\n                    link_tag = card.find(\"a\", href=True)\n                    link = link_tag[\"href\"] if link_tag else None\n                    if link and not link.startswith(\"http\"):\n                        link = urljoin(self.base_url, link)\n                    \n                    tipo_imovel = card.find(class_=\"card-property-price-lote\")\n                    address = card.find(class_=\"card-property-address\")\n                    \n                    prices = []\n                    price_blocks = card.find_all(\"ul\", class_=\"card-property-prices\")\n                    for block in price_blocks:\n                        items = block.find_all(\"li\", class_=\"card-property-price\")\n                        for item in items:\n                            label = item.find(class_=\"card-property-price-label\")\n                            value = item.find(class_=\"card-property-price-value\")\n                            date = item.find(class_=\"card-property-price-data\")\n                            \n                            if label and value and date:\n                                prices.append({\n                                    \"Tipo\": label.get_text(strip=True),\n                                    \"Valor\": value.get_text(strip=True).replace(\"R$\", \"\").replace(\".\", \"\").replace(\",\", \".\").strip(),\n                                    \"Data\": date.get_text(strip=True)\n                                })\n                    \n                    property_data = {\n                        \"tipo_imovel\": tipo_imovel.get_text(strip=True) if tipo_imovel else \"N/A\",\n                        \"endereco\": address.get_text(separator=\" \", strip=True) if address else \"N/A\",\n                        \"Link\": link if link else traceback.print_exc(),\n                        \"Preços\": prices if prices else [{\"Tipo\": \"N/A\", \"Valor\": \"N/A\", \"Data\": \"N/A\"}]\n                    }\n                    properties.append(property_data)\n                \n                except Exception as e:\n                    print(f\"[ERRO] Erro ao processar card na página principal: {str(e)}\")\n                    continue\n\n        except Exception as e:\n            print(f\"[ERRO] scrapMainPage: {str(e)}\")\n            traceback.print_exc()\n\n        return properties\n\n    def enrich_with_details(self, properties):\n        print(f\"Enriquecendo {len(properties)} propriedades com detalhes...\")\n\n        enriched_properties = [dict(prop) for prop in properties] \n        \n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            future_to_index = {\n                executor.submit(self.scrapItensPages, prop[\"Link\"]): i \n                for i, prop in enumerate(enriched_properties) \n                if self.is_valid_url(prop.get(\"Link\", \"\"))\n            }\n            \n            processed_count = 0\n            for future in as_completed(future_to_index):\n                original_index = future_to_index[future]\n                current_property_link = enriched_properties[original_index].get(\"Link\", \"URL desconhecida\")\n                processed_count += 1\n                try:\n                    details = future.result()\n                    if details: \n                        enriched_properties[original_index].update(details)\n                    else:\n                        print(f\"[AVISO] Nenhuma detalhe extraído para {current_property_link}. Mantendo dados parciais.\")\n                except Exception as e:\n                    print(f\"[ERRO] Falha ao enriquecer propriedade no índice {original_index} ({current_property_link}): {str(e)}\")\n                    traceback.print_exc()\n                    enriched_properties[original_index][\"ErroEnriquecimentoDetalhes\"] = str(e)\n                \n                if processed_count % 10 == 0 or processed_count == len(future_to_index):\n                    print(f\"Progresso de enriquecimento de detalhes: {processed_count}/{len(future_to_index)} propriedades processadas.\")\n        \n        return enriched_properties \n\n    def enrich_with_process_details(self, properties):\n        print(f\"Buscando detalhes de páginas de processo para {len(properties)} propriedades...\")\n        \n        tasks_for_process_pages = []\n        for prop in properties:\n            process_link = prop.get(\"Link do Processo Judicial\")\n            if process_link and self.is_valid_url(process_link):\n                tasks_for_process_pages.append((prop, process_link))\n\n        if not tasks_for_process_pages:\n            print(\"Nenhum link de processo válido encontrado para enriquecer.\")\n            return properties\n\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            future_to_prop = {\n                executor.submit(self._scrap_nested_page, link): prop_data\n                for prop_data, link in tasks_for_process_pages\n            }\n            \n            processed_count = 0\n            for future in as_completed(future_to_prop):\n                prop_data = future_to_prop[future] \n                processed_count += 1\n                try:\n                    nested_data = future.result() \n                    if nested_data:\n                        prop_data.update(nested_data) \n                except Exception as e:\n                    print(f\"[ERRO] Falha ao obter ou processar conteúdo da página de processo para {prop_data.get('Link')}: {e}\")\n                    traceback.print_exc()\n                    prop_data[\"ErroProcessoAninhado\"] = str(e)\n                \n                if processed_count % 10 == 0 or processed_count == len(tasks_for_process_pages):\n                    print(f\"Progresso de enriquecimento de links de processo: {processed_count}/{len(tasks_for_process_pages)} links processados.\")\n        return properties\n\n    def prepare_for_export(self, properties):\n        flat_properties = []\n        for prop in properties:\n            if \"Preços\" not in prop or not prop[\"Preços\"]:\n                base_prop = {\n                    \"Data\": \"\", \n                    \"endereco\": prop.get(\"endereco\", \"\"),\n                    \"Link\": prop.get(\"Link\", \"\"),\n                    \"tipo_imovel\": prop.get(\"tipo_imovel\", \"\"),\n                    \"rotulo\": \"N/A\", \n                    \"Valor (R$)\": \"N/A\" \n                }\n                for key, value in prop.items():\n                    if key not in [\"Preços\", \"Data\", \"endereco\", \"Link\", \"tipo_imovel\", \"rotulo\", \"Valor (R$)\"]:\n                        base_prop[key] = value\n                flat_properties.append(base_prop)\n            else:\n                for price in prop[\"Preços\"]:\n                    flat_prop = {\n                        \"Data\": price.get(\"Data\", \"\"),\n                        \"endereco\": prop.get(\"endereco\", \"\"),\n                        \"Link\": prop.get(\"Link\", \"\"),\n                        \"tipo_imovel\": prop.get(\"tipo_imovel\", \"\"),\n                        \"rotulo\": price.get(\"Tipo\", \"\"),\n                        \"Valor (R$)\": price.get(\"Valor\", \"\")\n                    }\n                    \n                    for key, value in prop.items():\n                        if key not in [\"Preços\", \"Data\", \"endereco\", \"Link\", \"tipo_imovel\", \"rotulo\", \"Valor (R$)\"]:\n                            flat_prop[key] = value\n                    \n                    flat_properties.append(flat_prop)\n        \n        return flat_properties\n\n    def export_to_csv(self, properties, filename=\"portalzuk.csv\"):\n        if not properties:\n            print(\"Nenhum dado para exportar (lista de propriedades vazia).\")\n            return False\n\n        flat_data = self.prepare_for_export(properties)\n        \n        if not flat_data:\n            print(\"Nenhum dado válido para exportação após preparo (lista achatada vazia).\")\n            return False\n\n        fieldnames = set()\n        for row in flat_data:\n            fieldnames.update(row.keys())\n        \n        preferred_order = [\n            \"tipo_imovel\", \"rotulo\", \"endereco\", \"Link\", \"Valor (R$)\", \n            \"Data\", \"Matrícula\", \"leiloeiro\", \"Descrição do imóvel\",\n            \"Formas de Pagamento\", \"Direito de Preferência\", \n            \"Observações\", \"Direitos do Compromissário\",\n            \"Link do Processo Judicial\", \"Visitação\"\n        ]\n        \n        photo_fields = sorted([f for f in fieldnames if f.startswith(\"Foto_\")], \n                              key=lambda x: int(x.split('_')[1]))\n        if photo_fields:\n            preferred_order.extend(photo_fields)\n            preferred_order.append(\"Total_Fotos\")\n\n        other_fieldnames = sorted([f for f in fieldnames if f not in preferred_order])\n        final_fieldnames = preferred_order + other_fieldnames\n        \n        try:\n            with open(filename, mode='w', newline='', encoding='utf-8-sig') as csvfile:\n                writer = csv.DictWriter(csvfile, fieldnames=final_fieldnames)\n                writer.writeheader()\n                writer.writerows(flat_data)\n            \n            print(f\"Dados exportados com sucesso para {filename}\")\n            print(f\"Total de registros no CSV: {len(flat_data)}\")\n            return True\n        except Exception as e:\n            print(f\"Erro ao exportar CSV: {str(e)}\")\n            traceback.print_exc()\n            return False\n\n    def run(self, start_url=None):\n        try:\n            print(\"Iniciando scraping com proteções contra bloqueio...\")\n            \n            start_time = time.time()\n            \n            target_url = start_url if start_url else self.base_url\n            \n            self.driver.get(target_url)\n            html=self.driver.page_source\n            properties=self.scrapMainPage(html)\n\n            print(f\"Propriedades encontradas na página principal: {len(properties)}\")\n            \n            if not properties:\n                print(\"Nenhuma propriedade encontrada na página principal para enriquecer. Encerrando.\")\n                return\n\n            enriched_properties = self.enrich_with_details(properties)\n            print(f\"Propriedades enriquecidas com detalhes da página do imóvel: {len(enriched_properties)}\")\n\n            final_properties = self.enrich_with_process_details(enriched_properties)\n            \n            parsed_url = urlparse(target_url)\n            filename_suffix = parsed_url.path.replace('/', '_').replace('-', '_').strip('_')\n            if not filename_suffix or filename_suffix == 'leilao_de_imoveis':\n                output_filename = \"portalzuk.csv\"\n            else:\n                output_filename = f\"portalzuk_{''.join(c if c.isalnum() else '_' for c in filename_suffix)}.csv\"\n\n            self.export_to_csv(final_properties, filename=output_filename)\n            \n            end_time = time.time()\n            print(f\"Processo concluído com sucesso em {end_time - start_time:.2f} segundos!\")\n            \n        except Exception as e:\n            print(f\"Erro durante a execução: {str(e)}\")\n            traceback.print_exc()\n        finally:\n            if self.driver:\n                self.driver.quit()\n\nif __name__ == \"__main__\":\n    scraper = PortalzukScraper()\n    scraper.run()"
  }
]