from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
import requests
from bs4 import BeautifulSoup
import logging

# Configuração básica
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 7, 15),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Definição da DAG
dag = DAG(
    'real_estate_scraper',
    default_args=default_args,
    description='Coleta diária de dados de imóveis',
    schedule_interval=timedelta(days=1),
    catchup=False,
)

def scrape_and_store():
    """Função principal que faz scraping e armazena no PostgreSQL"""
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    conn = pg_hook.get_conn()
    cursor = conn.cursor()
    
    # 1. Obter sites para monitorar
    cursor.execute("SELECT id, url, nome_site, localidade FROM imoveis_scraper.sites_monitorados WHERE ativo = TRUE")
    sites = cursor.fetchall()
    
    for site_id, url, nome_site, localidade in sites:
        try:
            logging.info(f"Processando: {url}")
            
            # 2. Fazer scraping (implementação mock - substitua pelo real)
            properties = mock_scrape(url, nome_site, localidade)
            
            # 3. Armazenar resultados
            table_name = f"imoveis_{nome_site.lower().replace('-', '_')}_{localidade.lower().replace('-', '_')}"
            
            for prop in properties:
                cursor.execute(f"""
                    INSERT INTO imoveis_scraper.{table_name} (
                        titulo, preco, endereco, bairro, cidade, area, 
                        quartos, banheiros, vagas, link, amenidades, 
                        condominio, tipo, scraping_date, source_url
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                    ) ON CONFLICT (link) DO NOTHING
                """, (
                    prop['titulo'], prop['preco'], prop['endereco'], 
                    prop['bairro'], prop['cidade'], prop['area'],
                    prop['quartos'], prop['banheiros'], prop['vagas'],
                    prop['link'], prop['amenidades'], prop['condominio'],
                    prop['tipo'], datetime.now(), url
                ))
            
            conn.commit()
            logging.info(f"Dados armazenados em {table_name}")
            
        except Exception as e:
            logging.error(f"Erro ao processar {url}: {str(e)}")
            conn.rollback()
    
    cursor.close()
    conn.close()

def mock_scrape(url, nome_site, localidade):
    """Implementação mock do scraper - substitua pelo real"""
    # Exemplo de dados mockados - implemente o scraping real aqui
    if 'zapimoveis' in url:
        return [{
            'titulo': f"Apartamento em {localidade.split('-')[0].title()}",
            'preco': 450000.00,
            'endereco': f"Rua Principal, 100 - {localidade.split('-')[0].title()}",
            'bairro': "Centro",
            'cidade': localidade.split('-')[0].title(),
            'area': 75,
            'quartos': 2,
            'banheiros': 2,
            'vagas': 1,
            'link': f"{url}/imovel-123",
            'amenidades': ["Piscina", "Academia"],
            'condominio': 800.00,
            'tipo': "Apartamento"
        }]
    else:
        return [{
            'titulo': f"Casa em {localidade.split('-')[0].title()}",
            'preco': 650000.00,
            'endereco': f"Av. Secundária, 200 - {localidade.split('-')[0].title()}",
            'bairro': "Bairro Nobre",
            'cidade': localidade.split('-')[0].title(),
            'area': 120,
            'quartos': 3,
            'banheiros': 3,
            'vagas': 2,
            'link': f"{url}/casa-456",
            'amenidades': ["Jardim", "Churrasqueira"],
            'condominio': 500.00,
            'tipo': "Casa"
        }]

# Definindo a tarefa principal
scrape_task = PythonOperator(
    task_id='scrape_and_store_task',
    python_callable=scrape_and_store,
    dag=dag,
)
